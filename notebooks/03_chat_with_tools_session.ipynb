{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f29407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aamirsyedaltaf/Documents/curator-pommeline/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:29,772\", \"name\": \"pinecone_index_client\", \"levelname\": \"INFO\", \"message\": \"Initialized PineconeIndexClient for dense index 'curator-pommeline' (dim: 768, metric: dotproduct)\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:29,772 - pinecone_index_client - INFO - Initialized PineconeIndexClient for dense index 'curator-pommeline' (dim: 768, metric: dotproduct)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:29,785\", \"name\": \"pinecone_vector_store\", \"levelname\": \"INFO\", \"message\": \"Connected to Pinecone Index container: {'namespaces': {'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, 'pommeline': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}, 'curator-pommeline': {'vectorCount': 212}, 'curator-pommeline-12fa085f': {'vectorCount': 0}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, '': {'vectorCount': 0}}, 'dimension': 768, 'indexFullness': 0.0, 'totalVectorCount': 212}\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:29,785 - pinecone_vector_store - INFO - Connected to Pinecone Index container: {'namespaces': {'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, 'pommeline': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}, 'curator-pommeline': {'vectorCount': 212}, 'curator-pommeline-12fa085f': {'vectorCount': 0}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, '': {'vectorCount': 0}}, 'dimension': 768, 'indexFullness': 0.0, 'totalVectorCount': 212}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:29,787\", \"name\": \"cache\", \"levelname\": \"INFO\", \"message\": \"Started cache cleanup task with 300s interval\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:29,787 - cache - INFO - Started cache cleanup task with 300s interval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all components\n",
      "LLMWithTools class: <class 'src.utils.llm_pipeline.LLMWithTools'>\n",
      "Retrieve function: <function retrieve_documents at 0x177dc0040>\n",
      "Search products function: <function search_products at 0x177dc0b80>\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from nest_asyncio import apply\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "apply()\n",
    "\n",
    "# Add parent directory to path for imports to handle relative imports\n",
    "sys.path.append(str(pathlib.Path().absolute().parent))\n",
    "sys.path.append(str(pathlib.Path().absolute().parent / \"src\"))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"chat_tool_calling_demo\")\n",
    "\n",
    "# Import required components\n",
    "from src.utils.llm_pipeline import LLMWithTools\n",
    "from src.tools.retrieve import retrieve_documents, RetrieveRequest\n",
    "from src.tools.search_product import search_products, ProductSearchRequest\n",
    "\n",
    "print(\"Successfully imported all components\")\n",
    "print(f\"LLMWithTools class: {LLMWithTools}\")\n",
    "print(f\"Retrieve function: {retrieve_documents}\")\n",
    "print(f\"Search products function: {search_products}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9cb3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tool functions...\n",
      "Testing retrieve_knowledge_tool...\n",
      "{\"asctime\": \"2025-10-30 13:35:29,897\", \"name\": \"embedder\", \"levelname\": \"INFO\", \"message\": \"Loading embedding model: google/embeddinggemma-300m\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:29,897 - embedder - INFO - Loading embedding model: google/embeddinggemma-300m\n",
      "2025-10-30 13:35:29,899 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: google/embeddinggemma-300m\n",
      "2025-10-30 13:35:38,050 - sentence_transformers.SentenceTransformer - INFO - 14 prompts are loaded, with the keys: ['query', 'document', 'BitextMining', 'Clustering', 'Classification', 'InstructionRetrieval', 'MultilabelClassification', 'PairClassification', 'Reranking', 'Retrieval', 'Retrieval-query', 'Retrieval-document', 'STS', 'Summarization']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:39,004\", \"name\": \"embedder\", \"levelname\": \"INFO\", \"message\": \"Model loaded successfully. Embedding dimension: 768\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:39,004 - embedder - INFO - Model loaded successfully. Embedding dimension: 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:39,131\", \"name\": \"bm25_vectorizer\", \"levelname\": \"ERROR\", \"message\": \"Failed to load BM25Vectorizer: [Errno 2] No such file or directory: 'data/models/bm25_d0a4624a.pkl'\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:39,131 - bm25_vectorizer - ERROR - Failed to load BM25Vectorizer: [Errno 2] No such file or directory: 'data/models/bm25_d0a4624a.pkl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:39,133\", \"name\": \"bm25_vectorizer\", \"levelname\": \"WARNING\", \"message\": \"BM25Vectorizer not found for index: d0a4624a\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:39,133 - bm25_vectorizer - WARNING - BM25Vectorizer not found for index: d0a4624a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:39,133\", \"name\": \"retrieve_tool\", \"levelname\": \"WARNING\", \"message\": \"No BM25 vectorizer found for ingestion ID: d0a4624a\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:39,133 - retrieve_tool - WARNING - No BM25 vectorizer found for ingestion ID: d0a4624a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve tool test: Found 3 documents\n",
      "First result: ### Hearing\n",
      "- **Mono Audio** for balanced listening\n",
      "- **Live Listen** with Made for iPhone hearing a...\n",
      "Testing search_products_tool...\n",
      "{\"asctime\": \"2025-10-30 13:35:39,137\", \"name\": \"search_product_tool\", \"levelname\": \"INFO\", \"message\": \"Searching products for query: 'iPhone'\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:39,137 - search_product_tool - INFO - Searching products for query: 'iPhone'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:39,274\", \"name\": \"search_product_tool\", \"levelname\": \"INFO\", \"message\": \"Found 3 products\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:39,274 - search_product_tool - INFO - Found 3 products\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search products tool test: Found 3 products\n",
      "First product: iPhone 16 Pro - $1059.690750260071\n",
      "\n",
      "Tool functions are defined (results may vary based on available data).\n"
     ]
    }
   ],
   "source": [
    "# Tool Function Definitions and Wrappers (Fixed Version - Correct API Usage + Clean Logging)\n",
    "\n",
    "def retrieve_knowledge_tool(query: str, top_k: int = 5, search_mode: str = \"hybrid\", similarity_threshold: float = 0.15) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tool function for retrieving knowledge base documents.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query for knowledge base (string)\n",
    "        top_k: Maximum number of documents to return (default: 5)\n",
    "        search_mode: Search mode - \"semantic\", \"keyword\", or \"hybrid\" (default: \"hybrid\")\n",
    "        similarity_threshold: Minimum similarity threshold (default: 0.15)\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing search results with documents and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Import here to avoid dependency issues\n",
    "        from src.tools.retrieve import retrieve_documents\n",
    "        \n",
    "        # Ensure parameters are of correct type\n",
    "        if not isinstance(query, str):\n",
    "            query = str(query) if query is not None else \"\"\n",
    "        \n",
    "        if not isinstance(top_k, int):\n",
    "            top_k = int(top_k) if top_k is not None else 5\n",
    "            \n",
    "        if not isinstance(search_mode, str):\n",
    "            search_mode = str(search_mode) if search_mode is not None else \"hybrid\"\n",
    "            \n",
    "        if not isinstance(similarity_threshold, (int, float)):\n",
    "            similarity_threshold = float(similarity_threshold) if similarity_threshold is not None else 0.15\n",
    "        \n",
    "        # Call retrieve_documents directly with string query + kwargs\n",
    "        response = retrieve_documents(\n",
    "            query=query,  # This must be a string\n",
    "            top_k=top_k,\n",
    "            search_mode=search_mode,\n",
    "            similarity_threshold=similarity_threshold,\n",
    "            include_scores=True\n",
    "        )\n",
    "        \n",
    "        # Format results for LLM consumption\n",
    "        results = []\n",
    "        for doc in response.results:\n",
    "            results.append({\n",
    "                \"content\": doc.content,\n",
    "                \"source_file\": doc.source_file,\n",
    "                \"score\": doc.score,\n",
    "                \"metadata\": doc.metadata\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"results\": results,\n",
    "            \"total_results\": len(results),\n",
    "            \"search_metadata\": response.search_metadata\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in retrieve_knowledge_tool: {e}\")\n",
    "        traceback.print_exc(limit=3)\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"results\": [],\n",
    "            \"total_results\": 0,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def search_products_tool(query: str, category: str = None, min_price: float = None, max_price: float = None, \n",
    "                        brand: str = None, limit: int = 10, sort_by: str = \"relevance\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tool function for searching product inventory.\n",
    "    \n",
    "    Args:\n",
    "        query: Product search query (string)\n",
    "        category: Filter by product category (string, optional)\n",
    "        min_price: Minimum price filter (number, optional)\n",
    "        max_price: Maximum price filter (number, optional)\n",
    "        brand: Filter by brand (string, optional)\n",
    "        limit: Maximum number of results (default: 10)\n",
    "        sort_by: Sort order - \"relevance\", \"price_low\", \"price_high\", \"rating\" (default: \"relevance\")\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing product search results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Import here to avoid dependency issues\n",
    "        from src.tools.search_product import search_products\n",
    "        \n",
    "        # Ensure parameters are of correct type\n",
    "        if not isinstance(query, str):\n",
    "            query = str(query) if query is not None else \"\"\n",
    "            \n",
    "        if category is not None and not isinstance(category, str):\n",
    "            category = str(category)\n",
    "            \n",
    "        if brand is not None and not isinstance(brand, str):\n",
    "            brand = str(brand)\n",
    "            \n",
    "        if limit is not None and not isinstance(limit, int):\n",
    "            limit = int(limit) if limit is not None else 10\n",
    "            \n",
    "        if sort_by is not None and not isinstance(sort_by, str):\n",
    "            sort_by = str(sort_by) if sort_by is not None else \"relevance\"\n",
    "            \n",
    "        if min_price is not None and not isinstance(min_price, (int, float)):\n",
    "            min_price = float(min_price) if min_price is not None else None\n",
    "            \n",
    "        if max_price is not None and not isinstance(max_price, (int, float)):\n",
    "            max_price = float(max_price) if max_price is not None else None\n",
    "        \n",
    "        # Call search_products directly with string query + kwargs\n",
    "        response = search_products(\n",
    "            query=query,  # This must be a string\n",
    "            category=category,\n",
    "            min_price=min_price,\n",
    "            max_price=max_price,\n",
    "            brand=brand,\n",
    "            limit=limit,\n",
    "            sort_by=sort_by\n",
    "        )\n",
    "        \n",
    "        # Format results for LLM consumption\n",
    "        products = []\n",
    "        for product in response.products:\n",
    "            products.append({\n",
    "                \"id\": product.id,\n",
    "                \"name\": product.name,\n",
    "                \"description\": product.description,\n",
    "                \"price\": product.price,\n",
    "                \"brand\": product.brand,\n",
    "                \"category\": product.category,\n",
    "                \"availability\": product.availability,\n",
    "                \"rating\": product.rating,\n",
    "                \"specifications\": product.specifications\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"products\": products,\n",
    "            \"total_results\": len(products),\n",
    "            \"filters_applied\": response.filters_applied,\n",
    "            \"search_metadata\": response.search_metadata\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in search_products_tool: {e}\")\n",
    "        traceback.print_exc(limit=3)\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"products\": [],\n",
    "            \"total_results\": 0,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Test the tool functions with direct calls\n",
    "print(\"Testing tool functions...\")\n",
    "\n",
    "# Test retrieve tool\n",
    "try:\n",
    "    print(\"Testing retrieve_knowledge_tool...\")\n",
    "    retrieve_result = retrieve_knowledge_tool(\"iPhone features\", top_k=3)\n",
    "    print(f\"Retrieve tool test: Found {retrieve_result['total_results']} documents\")\n",
    "    if retrieve_result['results']:\n",
    "        print(f\"First result: {retrieve_result['results'][0]['content'][:100]}...\")\n",
    "    else:\n",
    "        print(\"No documents found - this may be expected if no data is ingested\")\n",
    "except Exception as e:\n",
    "    print(f\"Retrieve tool test failed: {e}\")\n",
    "\n",
    "# Test search products tool\n",
    "try:\n",
    "    print(\"Testing search_products_tool...\")\n",
    "    search_result = search_products_tool(\"iPhone\", limit=3)\n",
    "    print(f\"Search products tool test: Found {search_result['total_results']} products\")\n",
    "    if search_result['products']:\n",
    "        print(f\"First product: {search_result['products'][0]['name']} - ${search_result['products'][0]['price']}\")\n",
    "    else:\n",
    "        print(\"No products found - this may be expected if no product data is available\")\n",
    "except Exception as e:\n",
    "    print(f\"Search products tool test failed: {e}\")\n",
    "\n",
    "print(\"\\nTool functions are defined (results may vary based on available data).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bebf141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt loaded successfully\n",
      "System prompt length: 3530 characters\n",
      "Tool schemas defined: 2 tools\n",
      "  - retrieve_knowledge: Search the knowledge base for product information, policies, and general informa...\n",
      "  - search_products: Search the product inventory for specific items with pricing and availability. U...\n"
     ]
    }
   ],
   "source": [
    "# Load System Instructions\n",
    "system_prompt = \"\"\n",
    "with open(\"../prompts/system_instructions.txt\", \"r\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "print(\"System prompt loaded successfully\")\n",
    "print(f\"System prompt length: {len(system_prompt)} characters\")\n",
    "\n",
    "# Define tool schemas for LLM\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"name\": \"retrieve_knowledge\",\n",
    "        \"description\": \"Search the knowledge base for product information, policies, and general information. Use this when you need factual information about products, return policies, or general knowledge.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query for the knowledge base\"\n",
    "                },\n",
    "                \"top_k\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of documents to return (default: 5)\",\n",
    "                    \"default\": 5\n",
    "                },\n",
    "                \"search_mode\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search mode: 'semantic' for conceptual search, 'keyword' for exact term matching, 'hybrid' for both (default: 'hybrid')\",\n",
    "                    \"enum\": [\"semantic\", \"keyword\", \"hybrid\"],\n",
    "                    \"default\": \"hybrid\"\n",
    "                },\n",
    "                \"similarity_threshold\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Minimum similarity threshold (default: 0.15)\",\n",
    "                    \"default\": 0.15\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_products\",\n",
    "        \"description\": \"Search the product inventory for specific items with pricing and availability. Use this when looking for specific products to buy or compare.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Product search query\"\n",
    "                },\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Filter by product category (e.g., 'Smartphones', 'Laptops', 'Audio')\"\n",
    "                },\n",
    "                \"min_price\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Minimum price filter\"\n",
    "                },\n",
    "                \"max_price\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Maximum price filter\"\n",
    "                },\n",
    "                \"brand\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Filter by brand (e.g., 'Apple', 'Samsung', 'Sony')\"\n",
    "                },\n",
    "                \"limit\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results (default: 10)\",\n",
    "                    \"default\": 10\n",
    "                },\n",
    "                \"sort_by\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Sort order: 'relevance', 'price_low', 'price_high', 'rating'\",\n",
    "                    \"enum\": [\"relevance\", \"price_low\", \"price_high\", \"rating\"],\n",
    "                    \"default\": \"relevance\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Tool schemas defined: {len(tools_schema)} tools\")\n",
    "for tool in tools_schema:\n",
    "    print(f\"  - {tool['name']}: {tool['description'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1cfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM with Tools...\n",
      "Using the real LLMWithTools class with tool calling capabilities...\n",
      "{\"asctime\": \"2025-10-30 13:35:39,298\", \"name\": \"src.utils.llm_pipeline\", \"levelname\": \"INFO\", \"message\": \"Initializing LLM singleton with model: glm-4.5-air\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:39,298 - src.utils.llm_pipeline - INFO - Initializing LLM singleton with model: glm-4.5-air\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM with Tools initialized successfully!\n",
      "Model: glm-4.5-air\n",
      "Tools registered: ['retrieve_knowledge', 'search_products']\n",
      "\n",
      "==================================================\n",
      "TESTING SINGLE MESSAGE WITH TOOL CALLING\n",
      "==================================================\n",
      "\n",
      "Test 1: What iPhones do you have available under $1000?\n",
      "----------------------------------------\n",
      "{\"asctime\": \"2025-10-30 13:35:41,859\", \"name\": \"search_product_tool\", \"levelname\": \"INFO\", \"message\": \"Searching products for query: 'iPhone'\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:41,859 - search_product_tool - INFO - Searching products for query: 'iPhone'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:42,052\", \"name\": \"search_product_tool\", \"levelname\": \"INFO\", \"message\": \"Found 1 products\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:42,052 - search_product_tool - INFO - Found 1 products\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the search results, I found one iPhone available under $1000:\n",
      "\n",
      "**iPhone 16 Pro** - $968.13\n",
      "- **Brand:** Apple\n",
      "- **Category:** Smartphones\n",
      "- **Status:** In Stock\n",
      "- **Rating:** 4.2/5\n",
      "\n",
      "**Key Specifications:**\n",
      "- Screen size: 6.3 inches\n",
      "- Storage: 256GB\n",
      "- Camera: 48MP main camera\n",
      "- Processor: A1...\n",
      "Latency: 5938.23ms\n",
      "\n",
      "Test 2: Tell me about the iPhone 16 Pro features\n",
      "----------------------------------------\n",
      "{\"asctime\": \"2025-10-30 13:35:48,157\", \"name\": \"retrieve_tool\", \"levelname\": \"WARNING\", \"message\": \"No BM25 vectorizer found for ingestion ID: d0a4624a\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:48,157 - retrieve_tool - WARNING - No BM25 vectorizer found for ingestion ID: d0a4624a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the information in our knowledge base, here are the key features of the iPhone 16 Pro:\n",
      "\n",
      "## Design and Display\n",
      "- **6.3-inch Super Retina XDR display** with ProMotion technology\n",
      "- **Titanium construction** for enhanced durability and reduced weight\n",
      "- **Ceramic Shield front** for improved drop...\n",
      "Latency: 7287.92ms\n",
      "\n",
      "Test 3: Show me products from Apple\n",
      "----------------------------------------\n",
      "{\"asctime\": \"2025-10-30 13:35:53,817\", \"name\": \"search_product_tool\", \"levelname\": \"INFO\", \"message\": \"Searching products for query: 'Apple'\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:53,817 - search_product_tool - INFO - Searching products for query: 'Apple'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 13:35:54,058\", \"name\": \"search_product_tool\", \"levelname\": \"INFO\", \"message\": \"Found 9 products\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:35:54,058 - search_product_tool - INFO - Found 9 products\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Here are the Apple products currently available in our inventory:\n",
      "\n",
      "## Smartphones\n",
      "\n",
      "### iPhone 16 Pro\n",
      "- **Price:** $968.13 - $1,059.69\n",
      "- **Description:** Latest iPhone with A18 Pro chip, titanium design, and advanced camera system\n",
      "- **Specifications:**\n",
      "  - 6.3 inches screen size\n",
      "  - 256GB storage\n",
      "  -...\n",
      "Latency: 7509.69ms\n",
      "\n",
      "Test 4: What's the difference between the 128GB and 256GB models?\n",
      "----------------------------------------\n",
      "Response: I need more information about which specific product you're asking about. Could you please let me know which device or product you're comparing? For example, are you asking about:\n",
      "\n",
      "- An iPhone model?\n",
      "- An Android phone?\n",
      "- A laptop?\n",
      "- A tablet?\n",
      "- Another storage device?\n",
      "\n",
      "Once you specify the product,...\n",
      "Latency: 3162.08ms\n",
      "\n",
      "Single message testing completed!\n",
      "✅ Real LLMWithTools class with tool calling is working!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM with Tools (Using Real LLMWithTools Class)\n",
    "\n",
    "print(\"Initializing LLM with Tools...\")\n",
    "print(\"Using the real LLMWithTools class with tool calling capabilities...\")\n",
    "\n",
    "# Create LLM with tools instance using the real class\n",
    "llm_with_tools = LLMWithTools(\n",
    "    system_prompt=system_prompt,\n",
    "    model=\"glm-4.5-air\",\n",
    "    tools=tools_schema,\n",
    "    tool_choice=\"auto\",\n",
    "    max_timeout_per_request=60\n",
    ")\n",
    "\n",
    "# Register real tool functions\n",
    "llm_with_tools.register_function(\"retrieve_knowledge\", retrieve_knowledge_tool)\n",
    "llm_with_tools.register_function(\"search_products\", search_products_tool)\n",
    "\n",
    "print(\"LLM with Tools initialized successfully!\")\n",
    "print(f\"Model: {llm_with_tools.model}\")\n",
    "print(f\"Tools registered: {list(llm_with_tools.available_functions.keys())}\")\n",
    "\n",
    "# Test single message with tools\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING SINGLE MESSAGE WITH TOOL CALLING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_queries = [\n",
    "    \"What iPhones do you have available under $1000?\",\n",
    "    \"Tell me about the iPhone 16 Pro features\",\n",
    "    \"Show me products from Apple\",\n",
    "    \"What's the difference between the 128GB and 256GB models?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nTest {i}: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Generate response with tool execution\n",
    "        response = await llm_with_tools.generate_with_tool_execution(\n",
    "            user_prompt=query,\n",
    "            max_retries=2,\n",
    "            max_tool_iterations=3\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "        \n",
    "        if response[\"type\"] == \"text\":\n",
    "            print(f\"Response: {response['content'][:300]}...\")\n",
    "            print(f\"Latency: {latency:.2f}ms\")\n",
    "        elif response[\"type\"] == \"tool_calls\":\n",
    "            print(f\"Tool calls requested: {[tc['name'] for tc in response.get('tool_calls', [])]}\")\n",
    "            print(f\"Response preview: {response.get('content', '')[:200]}...\")\n",
    "            print(f\"Latency: {latency:.2f}ms\")\n",
    "        elif response[\"type\"] == \"error\":\n",
    "            print(f\"Error: {response['content']}\")\n",
    "            print(f\"Latency: {latency:.2f}ms\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc(limit=3))\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        print(f\"Failed latency: {latency:.2f}ms\")\n",
    "\n",
    "print(\"\\nSingle message testing completed!\")\n",
    "print(\"✅ Real LLMWithTools class with tool calling is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatBot Class with Real Tool Integration\n",
    "\n",
    "@dataclass\n",
    "class ChatMessage:\n",
    "    \"\"\"Message data structure for chat history.\"\"\"\n",
    "    role: str  # \"user\", \"assistant\", \"system\"\n",
    "    content: str\n",
    "    timestamp: str\n",
    "    latency_ms: Optional[float] = None\n",
    "    tools_used: Optional[List[str]] = None\n",
    "\n",
    "class ChatBotWithTools:\n",
    "    \"\"\"\n",
    "    Advanced chatbot class with real tool integration and conversation history.\n",
    "    \n",
    "    Features:\n",
    "    - Single message queries with automatic tool execution\n",
    "    - Multi-turn conversations with context persistence\n",
    "    - Real tool calling (retrieve_knowledge, search_products)\n",
    "    - Latency tracking and optimization\n",
    "    - Message history management\n",
    "    - Error handling and recovery\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_with_tools, max_history_length: int = 20):\n",
    "        \"\"\"\n",
    "        Initialize the ChatBot with tools.\n",
    "        \n",
    "        Args:\n",
    "            llm_with_tools: Configured LLMWithTools instance\n",
    "            max_history_length: Maximum number of messages to keep in history\n",
    "        \"\"\"\n",
    "        self.llm = llm_with_tools\n",
    "        self.max_history_length = max_history_length\n",
    "        self.conversation_history: List[ChatMessage] = []\n",
    "        self.session_start_time = datetime.now()\n",
    "        \n",
    "        print(f\"ChatBot initialized with {len(llm_with_tools.available_functions)} tools\")\n",
    "        print(f\"Available tools: {list(llm_with_tools.available_functions.keys())}\")\n",
    "        print(f\"Max history length: {max_history_length} messages\")\n",
    "    \n",
    "    async def single_turn_chat(self, user_message: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a single message without conversation history.\n",
    "        Tools are automatically called based on user query.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The user's input message\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing response and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        tools_used = []\n",
    "        \n",
    "        try:\n",
    "            # Use LLM with automatic tool execution\n",
    "            response = await self.llm.generate_with_tool_execution(\n",
    "                user_prompt=user_message,\n",
    "                max_retries=2,\n",
    "                max_tool_iterations=3\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            \n",
    "            if response[\"type\"] == \"text\":\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"response\": response[\"content\"],\n",
    "                    \"latency_ms\": latency_ms,\n",
    "                    \"tools_used\": tools_used,\n",
    "                    \"message_type\": \"single_turn\",\n",
    "                    \"tool_calls_made\": 0\n",
    "                }\n",
    "            elif response[\"type\"] == \"tool_calls\":\n",
    "                # Tools were called during execution\n",
    "                tools_used = [tc[\"name\"] for tc in response.get(\"tool_calls\", [])]\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"response\": response.get(\"content\", \"Tool execution completed.\"),\n",
    "                    \"latency_ms\": latency_ms,\n",
    "                    \"tools_used\": tools_used,\n",
    "                    \"message_type\": \"single_turn\",\n",
    "                    \"tool_calls_made\": len(tools_used)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": response.get(\"content\", \"Unknown error\"),\n",
    "                    \"latency_ms\": latency_ms,\n",
    "                    \"tools_used\": tools_used,\n",
    "                    \"message_type\": \"single_turn\",\n",
    "                    \"tool_calls_made\": 0\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"latency_ms\": latency_ms,\n",
    "                \"tools_used\": tools_used,\n",
    "                \"message_type\": \"single_turn\",\n",
    "                \"tool_calls_made\": 0\n",
    "            }\n",
    "    \n",
    "    async def multi_turn_chat(self, user_message: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a message with conversation history for multi-turn conversations.\n",
    "        Context and previous tool results are considered.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The user's input message\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing response and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        tools_used = []\n",
    "        \n",
    "        # Add user message to history\n",
    "        user_chat_message = ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=user_message,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "        self.conversation_history.append(user_chat_message)\n",
    "        \n",
    "        try:\n",
    "            # Build conversation context from history\n",
    "            conversation_context = self._build_conversation_context()\n",
    "            \n",
    "            # Get response from LLM with conversation context and tool execution\n",
    "            response = await self.llm.generate_with_tool_execution(\n",
    "                user_prompt=conversation_context,\n",
    "                max_retries=2,\n",
    "                max_tool_iterations=3\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            \n",
    "            if response[\"type\"] == \"text\":\n",
    "                # Add assistant response to history\n",
    "                assistant_message = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=response[\"content\"],\n",
    "                    timestamp=datetime.now().isoformat(),\n",
    "                    latency_ms=latency_ms,\n",
    "                    tools_used=tools_used\n",
    "                )\n",
    "                self.conversation_history.append(assistant_message)\n",
    "                \n",
    "                # Trim history if needed\n",
    "                self._trim_history()\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"response\": response[\"content\"],\n",
    "                    \"latency_ms\": latency_ms,\n",
    "                    \"tools_used\": tools_used,\n",
    "                    \"message_type\": \"multi_turn\",\n",
    "                    \"history_length\": len(self.conversation_history),\n",
    "                    \"tool_calls_made\": 0\n",
    "                }\n",
    "            elif response[\"type\"] == \"tool_calls\":\n",
    "                # Tools were called during execution\n",
    "                tools_used = [tc[\"name\"] for tc in response.get(\"tool_calls\", [])]\n",
    "                \n",
    "                # Add assistant response to history\n",
    "                assistant_message = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=response.get(\"content\", \"Tool execution completed.\"),\n",
    "                    timestamp=datetime.now().isoformat(),\n",
    "                    latency_ms=latency_ms,\n",
    "                    tools_used=tools_used\n",
    "                )\n",
    "                self.conversation_history.append(assistant_message)\n",
    "                \n",
    "                # Trim history if needed\n",
    "                self._trim_history()\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"response\": response.get(\"content\", \"Tool execution completed.\"),\n",
    "                    \"latency_ms\": latency_ms,\n",
    "                    \"tools_used\": tools_used,\n",
    "                    \"message_type\": \"multi_turn\",\n",
    "                    \"history_length\": len(self.conversation_history),\n",
    "                    \"tool_calls_made\": len(tools_used)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": response.get(\"content\", \"Unknown error\"),\n",
    "                    \"latency_ms\": latency_ms,\n",
    "                    \"tools_used\": tools_used,\n",
    "                    \"message_type\": \"multi_turn\",\n",
    "                    \"history_length\": len(self.conversation_history),\n",
    "                    \"tool_calls_made\": 0\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"latency_ms\": latency_ms,\n",
    "                \"tools_used\": tools_used,\n",
    "                \"message_type\": \"multi_turn\",\n",
    "                \"history_length\": len(self.conversation_history),\n",
    "                \"tool_calls_made\": 0\n",
    "            }\n",
    "    \n",
    "    def _build_conversation_context(self) -> str:\n",
    "        \"\"\"Build conversation context from message history.\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        # Include recent conversation history (exclude system message)\n",
    "        recent_messages = [msg for msg in self.conversation_history if msg.role != \"system\"]\n",
    "        \n",
    "        # Limit to last 6 exchanges for better context with tools\n",
    "        context_messages = recent_messages[-6:]\n",
    "        \n",
    "        for msg in context_messages:\n",
    "            if msg.role == \"user\":\n",
    "                context_parts.append(f\"User: {msg.content}\")\n",
    "            elif msg.role == \"assistant\":\n",
    "                context_parts.append(f\"Assistant: {msg.content}\")\n",
    "        \n",
    "        # Add the latest user message at the end if not already included\n",
    "        if context_parts:\n",
    "            latest_user_msg = [msg for msg in self.conversation_history if msg.role == \"user\"][-1]\n",
    "            if not context_parts[-1].startswith(f\"User: {latest_user_msg.content}\"):\n",
    "                context_parts.append(f\"User: {latest_user_msg.content}\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _trim_history(self):\n",
    "        \"\"\"Trim conversation history to maintain maximum length.\"\"\"\n",
    "        if len(self.conversation_history) > self.max_history_length:\n",
    "            # Keep system message and recent messages\n",
    "            system_messages = [msg for msg in self.conversation_history if msg.role == \"system\"]\n",
    "            other_messages = [msg for msg in self.conversation_history if msg.role != \"system\"]\n",
    "            \n",
    "            # Keep most recent messages\n",
    "            recent_other_messages = other_messages[-(self.max_history_length - len(system_messages)):]\n",
    "            \n",
    "            self.conversation_history = system_messages + recent_other_messages\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get formatted conversation history.\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"role\": msg.role,\n",
    "                \"content\": msg.content,\n",
    "                \"timestamp\": msg.timestamp,\n",
    "                \"latency_ms\": msg.latency_ms,\n",
    "                \"tools_used\": msg.tools_used or []\n",
    "            }\n",
    "            for msg in self.conversation_history\n",
    "        ]\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history but keep system message.\"\"\"\n",
    "        system_messages = [msg for msg in self.conversation_history if msg.role == \"system\"]\n",
    "        self.conversation_history = system_messages\n",
    "        print(\"Conversation history cleared\")\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get chatbot session statistics.\"\"\"\n",
    "        user_messages = [msg for msg in self.conversation_history if msg.role == \"user\"]\n",
    "        assistant_messages = [msg for msg in self.conversation_history if msg.role == \"assistant\"]\n",
    "        \n",
    "        total_latencies = [msg.latency_ms for msg in assistant_messages if msg.latency_ms]\n",
    "        avg_latency = sum(total_latencies) / len(total_latencies) if total_latencies else 0\n",
    "        \n",
    "        # Count tool usage\n",
    "        all_tools_used = []\n",
    "        for msg in assistant_messages:\n",
    "            if msg.tools_used:\n",
    "                all_tools_used.extend(msg.tools_used)\n",
    "        \n",
    "        tool_usage_counts = {}\n",
    "        for tool in all_tools_used:\n",
    "            tool_usage_counts[tool] = tool_usage_counts.get(tool, 0) + 1\n",
    "        \n",
    "        session_duration = (datetime.now() - self.session_start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"session_duration_seconds\": session_duration,\n",
    "            \"total_messages\": len(user_messages) + len(assistant_messages),\n",
    "            \"user_messages\": len(user_messages),\n",
    "            \"assistant_messages\": len(assistant_messages),\n",
    "            \"average_latency_ms\": round(avg_latency, 2),\n",
    "            \"tools_available\": list(self.llm.available_functions.keys()),\n",
    "            \"tool_usage_counts\": tool_usage_counts,\n",
    "            \"total_tool_calls\": sum(tool_usage_counts.values()),\n",
    "            \"history_length\": len(self.conversation_history)\n",
    "        }\n",
    "\n",
    "# Initialize ChatBot with real tools\n",
    "print(\"Initializing ChatBot with real tool integration...\")\n",
    "chatbot = ChatBotWithTools(llm_with_tools, max_history_length=15)\n",
    "print(\"ChatBot with real tools initialized successfully!\")\n",
    "\n",
    "# Display initial stats\n",
    "stats = chatbot.get_stats()\n",
    "print(f\"Session stats: {stats}\")\n",
    "\n",
    "print(\"\\n✅ ChatBot with real tool integration is working!\")\n",
    "print(\"✅ Ready for testing single and multi-turn conversations with automatic tool calling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tz4jk0ojfn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Demo and Usage Examples with Real Tool Integration\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERACTIVE CHATBOT DEMO WITH REAL TOOLS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "async def interactive_demo():\n",
    "    \"\"\"Interactive demonstration of the chatbot with real tool integration.\"\"\"\n",
    "    \n",
    "    print(\"\\nThis interactive demo allows you to test the chatbot with real tool integration.\")\n",
    "    print(\"The chatbot will automatically call tools based on your queries:\")\n",
    "    print(\"- 📋 retrieve_knowledge: Search knowledge base for product information, policies\")\n",
    "    print(\"- 🛒 search_products: Search product inventory with pricing and availability\")\n",
    "    print(\"\\nCommands: 'quit' to exit, 'stats' for session stats, 'history' for conversation history, 'clear' to clear history\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    conversation_mode = \"single\"  # Start with single turn mode\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "                \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "                \n",
    "            if user_input.lower() == 'stats':\n",
    "                stats = chatbot.get_stats()\n",
    "                print(\"\\nSession Statistics:\")\n",
    "                for key, value in stats.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "                continue\n",
    "                \n",
    "            if user_input.lower() == 'history':\n",
    "                history = chatbot.get_conversation_history()\n",
    "                print(f\"\\nConversation History ({len(history)} messages):\")\n",
    "                for i, msg in enumerate(history):\n",
    "                    if msg['role'] != 'system':  # Skip system messages\n",
    "                        timestamp = msg['timestamp'][:19]\n",
    "                        role = msg['role'].upper()\n",
    "                        content_preview = msg['content'][:150] + \"...\" if len(msg['content']) > 150 else msg['content']\n",
    "                        tools_info = f\" [Tools: {', '.join(msg['tools_used'])}]\" if msg['tools_used'] else \"\"\n",
    "                        print(f\"  [{timestamp}] {role}: {content_preview}{tools_info}\")\n",
    "                continue\n",
    "                \n",
    "            if user_input.lower() == 'clear':\n",
    "                chatbot.clear_history()\n",
    "                print(\"Conversation history cleared!\")\n",
    "                continue\n",
    "                \n",
    "            if user_input.lower() == 'mode':\n",
    "                conversation_mode = \"multi\" if conversation_mode == \"single\" else \"single\"\n",
    "                print(f\"Switched to {conversation_mode}-turn mode\")\n",
    "                continue\n",
    "            \n",
    "            # Process the message\n",
    "            print(f\"\\nProcessing with {conversation_mode}-turn mode...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if conversation_mode == \"single\":\n",
    "                result = await chatbot.single_turn_chat(user_input)\n",
    "            else:\n",
    "                result = await chatbot.multi_turn_chat(user_input)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Display response\n",
    "            if result[\"success\"]:\n",
    "                print(f\"\\nAssistant: {result['response']}\")\n",
    "                \n",
    "                # Show tool usage information\n",
    "                tool_info = []\n",
    "                if result[\"tool_calls_made\"] > 0:\n",
    "                    tool_info.append(f\"Tools called: {result['tool_calls_made']}\")\n",
    "                if result[\"tools_used\"]:\n",
    "                    tool_info.append(f\"Tools used: {', '.join(result['tools_used'])}\")\n",
    "                \n",
    "                latency_info = f\"Latency: {result['latency_ms']:.2f}ms\"\n",
    "                if tool_info:\n",
    "                    latency_info += f\" | {', '.join(tool_info)}\"\n",
    "                \n",
    "                print(f\"\\n[{latency_info}]\")\n",
    "                \n",
    "                if result[\"message_type\"] == \"multi-turn\":\n",
    "                    print(f\"[History length: {result['history_length']} messages]\")\n",
    "            else:\n",
    "                print(f\"\\nError: {result['error']}\")\n",
    "                print(f\"[Latency: {result['latency_ms']:.2f}ms]\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nGoodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "\n",
    "# Comprehensive Performance Testing with Real Tools\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE TESTING WITH REAL TOOL INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "async def comprehensive_performance_test():\n",
    "    \"\"\"Run comprehensive performance tests on the chatbot with real tools.\"\"\"\n",
    "    \n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Product Search (should use search_products)\",\n",
    "            \"query\": \"What iPhones do you have available under $1000?\",\n",
    "            \"expected_tools\": [\"search_products\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Product Features (should use retrieve_knowledge)\",\n",
    "            \"query\": \"What are the key features of iPhone 16 Pro?\",\n",
    "            \"expected_tools\": [\"retrieve_knowledge\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Brand Search (should use search_products)\",\n",
    "            \"query\": \"Show me all Apple products\",\n",
    "            \"expected_tools\": [\"search_products\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Policy Query (should use retrieve_knowledge)\",\n",
    "            \"query\": \"What is your return policy?\",\n",
    "            \"expected_tools\": [\"retrieve_knowledge\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Complex Query (might use multiple tools)\",\n",
    "            \"query\": \"I need a phone for photography with good battery life under $800. Compare options and tell me about return policies.\",\n",
    "            \"expected_tools\": [\"search_products\", \"retrieve_knowledge\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Running comprehensive performance tests...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    single_turn_results = []\n",
    "    multi_turn_results = []\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"\\nTesting: {scenario['name']}\")\n",
    "        print(f\"Query: {scenario['query']}\")\n",
    "        print(f\"Expected tools: {scenario['expected_tools']}\")\n",
    "        \n",
    "        # Test single turn\n",
    "        print(\"Testing single-turn...\")\n",
    "        start = time.time()\n",
    "        result_single = await chatbot.single_turn_chat(scenario['query'])\n",
    "        single_latency = time.time() - start\n",
    "        \n",
    "        # Test multi turn (clear history first for fair comparison)\n",
    "        chatbot.clear_history()\n",
    "        print(\"Testing multi-turn...\")\n",
    "        start = time.time()\n",
    "        result_multi = await chatbot.multi_turn_chat(scenario['query'])\n",
    "        multi_latency = time.time() - start\n",
    "        \n",
    "        # Store results\n",
    "        single_turn_results.append({\n",
    "            \"scenario\": scenario['name'],\n",
    "            \"latency\": single_latency * 1000,\n",
    "            \"success\": result_single['success'],\n",
    "            \"tools_used\": result_single.get('tools_used', []),\n",
    "            \"tool_calls\": result_single.get('tool_calls_made', 0)\n",
    "        })\n",
    "        \n",
    "        multi_turn_results.append({\n",
    "            \"scenario\": scenario['name'],\n",
    "            \"latency\": multi_latency * 1000,\n",
    "            \"success\": result_multi['success'],\n",
    "            \"tools_used\": result_multi.get('tools_used', []),\n",
    "            \"tool_calls\": result_multi.get('tool_calls_made', 0)\n",
    "        })\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"✅ Single-turn: {single_latency*1000:.2f}ms - Success: {result_single['success']}\")\n",
    "        if result_single['success']:\n",
    "            print(f\"   Tools used: {result_single.get('tools_used', [])}\")\n",
    "            print(f\"   Tool calls made: {result_single.get('tool_calls_made', 0)}\")\n",
    "            response_preview = result_single['response'][:100] + \"...\"\n",
    "            print(f\"   Response preview: {response_preview}\")\n",
    "        \n",
    "        print(f\"✅ Multi-turn: {multi_latency*1000:.2f}ms - Success: {result_multi['success']}\")\n",
    "        if result_multi['success']:\n",
    "            print(f\"   Tools used: {result_multi.get('tools_used', [])}\")\n",
    "            print(f\"   Tool calls made: {result_multi.get('tool_calls_made', 0)}\")\n",
    "    \n",
    "    # Performance Analysis\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_single_latency = sum(r['latency'] for r in single_turn_results) / len(single_turn_results)\n",
    "    avg_multi_latency = sum(r['latency'] for r in multi_turn_results) / len(multi_turn_results)\n",
    "    \n",
    "    successful_single = sum(1 for r in single_turn_results if r['success'])\n",
    "    successful_multi = sum(1 for r in multi_turn_results if r['success'])\n",
    "    \n",
    "    total_tool_calls_single = sum(r['tool_calls'] for r in single_turn_results)\n",
    "    total_tool_calls_multi = sum(r['tool_calls'] for r in multi_turn_results)\n",
    "    \n",
    "    print(f\"Success Rate:\")\n",
    "    print(f\"  Single-turn: {successful_single}/{len(single_turn_results)} ({successful_single/len(single_turn_results)*100:.1f}%)\")\n",
    "    print(f\"  Multi-turn: {successful_multi}/{len(multi_turn_results)} ({successful_multi/len(multi_turn_results)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nAverage Latency:\")\n",
    "    print(f\"  Single-turn: {avg_single_latency:.2f}ms\")\n",
    "    print(f\"  Multi-turn: {avg_multi_latency:.2f}ms\")\n",
    "    print(f\"  Overhead: {abs(avg_multi_latency - avg_single_latency):.2f}ms ({abs(avg_multi_latency - avg_single_latency)/avg_single_latency*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTool Usage:\")\n",
    "    print(f\"  Total tool calls (single-turn): {total_tool_calls_single}\")\n",
    "    print(f\"  Total tool calls (multi-turn): {total_tool_calls_multi}\")\n",
    "    \n",
    "    # Tool usage breakdown\n",
    "    tool_usage_single = {}\n",
    "    tool_usage_multi = {}\n",
    "    \n",
    "    for result in single_turn_results:\n",
    "        for tool in result['tools_used']:\n",
    "            tool_usage_single[tool] = tool_usage_single.get(tool, 0) + 1\n",
    "    \n",
    "    for result in multi_turn_results:\n",
    "        for tool in result['tools_used']:\n",
    "            tool_usage_multi[tool] = tool_usage_multi.get(tool, 0) + 1\n",
    "    \n",
    "    print(f\"\\nTool Usage Breakdown:\")\n",
    "    all_tools = set(tool_usage_single.keys()) | set(tool_usage_multi.keys())\n",
    "    for tool in all_tools:\n",
    "        single_count = tool_usage_single.get(tool, 0)\n",
    "        multi_count = tool_usage_multi.get(tool, 0)\n",
    "        print(f\"  {tool}: {single_count} (single) / {multi_count} (multi)\")\n",
    "    \n",
    "    print(f\"\\n✅ Performance testing completed!\")\n",
    "    print(f\"✅ Tool integration is working correctly!\")\n",
    "\n",
    "# Run comprehensive performance test\n",
    "await comprehensive_performance_test()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"READY FOR INTERACTIVE USE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTo start interactive demo, run:\")\n",
    "print(\"await interactive_demo()\")\n",
    "print(\"\\nTo test individual queries:\")\n",
    "print(\"result = await chatbot.single_turn_chat('your query here')\")\n",
    "print(\"or\")\n",
    "print(\"result = await chatbot.multi_turn_chat('your query here')\")\n",
    "\n",
    "print(\"\\nChatBot Features:\")\n",
    "print(\"✅ Real tool integration (retrieve_knowledge, search_products)\")\n",
    "print(\"✅ Automatic tool calling based on query analysis\")\n",
    "print(\"✅ Single-turn chat with tool execution\")\n",
    "print(\"✅ Multi-turn chat with conversation context\")\n",
    "print(\"✅ Latency tracking and optimization\")\n",
    "print(\"✅ Conversation persistence with tool context\")\n",
    "print(\"✅ Error handling and recovery\")\n",
    "print(\"✅ Session statistics and tool usage analytics\")\n",
    "print(\"✅ Comprehensive performance monitoring\")\n",
    "\n",
    "# Display current session stats\n",
    "current_stats = chatbot.get_stats()\n",
    "print(f\"\\nCurrent session stats: {current_stats}\")\n",
    "\n",
    "print(\"\\nExample queries to try:\")\n",
    "print(\"- 'What iPhones do you have under $1000?'\")\n",
    "print(\"- 'Tell me about iPhone 16 Pro features'\")\n",
    "print(\"- 'What is your return policy?'\")\n",
    "print(\"- 'Show me Apple products with good ratings'\")\n",
    "print(\"- 'Compare laptops and tell me about warranty policies'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curator-pommeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

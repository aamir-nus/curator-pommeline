{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion Demo for Pommeline Product Knowledge Base\n",
    "\n",
    "This notebook demonstrates how to ingest product documents into the Pinecone vector store for the Pommeline knowledge base.\n",
    "\n",
    "## Features:\n",
    "- Checks for existing 'pommeline' index in Pinecone\n",
    "- Creates index if it doesn't exist with HNSW algorithm and dotproduct similarity\n",
    "- Ingests product documents with proper chunking and embedding\n",
    "- Normalizes embeddings before storage for optimal retrieval\n",
    "- Provides detailed logging and progress tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cea68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv add pinecone-client sentence-transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d155f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aamirsyedaltaf/Documents/curator-pommeline/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:51,987\", \"name\": \"pinecone_index_client\", \"levelname\": \"INFO\", \"message\": \"Initialized PineconeIndexClient for dense index 'curator-pommeline' (dim: 768, metric: dotproduct)\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:51,987 - pinecone_index_client - INFO - Initialized PineconeIndexClient for dense index 'curator-pommeline' (dim: 768, metric: dotproduct)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,003\", \"name\": \"pinecone_vector_store\", \"levelname\": \"INFO\", \"message\": \"Connected to Pinecone Index container: {'namespaces': {'curator-pommeline-12fa085f': {'vectorCount': 0}, 'pommeline': {'vectorCount': 0}, 'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, '': {'vectorCount': 0}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}, 'curator-pommeline': {'vectorCount': 92}}, 'dimension': 768, 'indexFullness': 0.0, 'totalVectorCount': 92}\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,003 - pinecone_vector_store - INFO - Connected to Pinecone Index container: {'namespaces': {'curator-pommeline-12fa085f': {'vectorCount': 0}, 'pommeline': {'vectorCount': 0}, 'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, '': {'vectorCount': 0}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}, 'curator-pommeline': {'vectorCount': 92}}, 'dimension': 768, 'indexFullness': 0.0, 'totalVectorCount': 92}\n",
      "2025-10-30 03:28:52,013 - ingestion_demo - INFO - Successfully imported all required modules\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add parent directory to path for imports to handle relative imports\n",
    "sys.path.append(str(pathlib.Path().absolute().parent))\n",
    "sys.path.append(str(pathlib.Path().absolute().parent / \"src\"))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"ingestion_demo\")\n",
    "\n",
    "# Import our modules\n",
    "from src.ingestion.vector_store import get_vector_store\n",
    "from src.ingestion.chunker import SemanticChunker, DocumentChunk\n",
    "from src.ingestion.embedder import EmbeddingGenerator\n",
    "from src.utils.file_loader import load_documents_from_directory\n",
    "from src.config import settings\n",
    "\n",
    "logger.info(\"Successfully imported all required modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f2c0a",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the index configuration for the Pommeline knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66fed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated unique index UUID: ccb01621\n",
      "Index configuration: curator-pommeline-ccb01621\n",
      "Dimension: 768, Metric: dotproduct\n",
      "Note: This index will be automatically cleaned up at the end of the notebook.\n"
     ]
    }
   ],
   "source": [
    "# Index configuration with UUID for unique identification\n",
    "import uuid\n",
    "\n",
    "# Generate a unique UUID for this notebook run\n",
    "index_uuid = str(uuid.uuid4())[:8]\n",
    "INDEX_NAME = f\"curator-pommeline-{index_uuid}\"\n",
    "DIMENSION = 768\n",
    "METRIC = \"dotproduct\"\n",
    "\n",
    "# Update settings for our specific index\n",
    "settings.pinecone_index_name = INDEX_NAME\n",
    "settings.pinecone_dimension = DIMENSION\n",
    "settings.pinecone_metric = METRIC\n",
    "\n",
    "print(f\"Generated unique index UUID: {index_uuid}\")\n",
    "print(f\"Index configuration: {INDEX_NAME}\")\n",
    "print(f\"Dimension: {DIMENSION}, Metric: {METRIC}\")\n",
    "print(f\"Note: This index will be automatically cleaned up at the end of the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5a959",
   "metadata": {},
   "source": [
    "## Initialize Vector Store\n",
    "\n",
    "Connect to Pinecone and set up the 'pommeline' index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef384b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Status:\n",
      "  total_documents: 0\n",
      "  embedding_dimension: 768\n",
      "  index_name: curator-pommeline\n",
      "  index_fullness: 0\n",
      "  index_type: pinecone_index_container\n",
      "  namespaces: {'': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}, 'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, 'pommeline': {'vectorCount': 0}, 'curator-pommeline': {'vectorCount': 92}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, 'curator-pommeline-12fa085f': {'vectorCount': 0}}\n",
      "IMPORTANT: Using unified index architecture\n",
      "Index configured as: 'curator-pommeline'\n",
      "Ingestion will use namespace: 'curator-pommeline-ccb01621'\n",
      "This creates proper unified index with dense+sparse vectors\n",
      "This index will be automatically cleaned up at the end of the notebook.\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store with our unique configuration\n",
    "vector_store = get_vector_store()\n",
    "\n",
    "# Check current status\n",
    "stats = vector_store.get_stats()\n",
    "print(\"Vector Store Status:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"IMPORTANT: Using unified index architecture\")\n",
    "print(f\"Index configured as: '{stats['index_name']}'\")\n",
    "print(f\"Ingestion will use namespace: '{INDEX_NAME}'\")\n",
    "print(f\"This creates proper unified index with dense+sparse vectors\")\n",
    "print(f\"This index will be automatically cleaned up at the end of the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615734a",
   "metadata": {},
   "source": [
    "## Load Product Documents\n",
    "\n",
    "Load all product and policy documents from the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26338f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from: /Users/aamirsyedaltaf/Documents/curator-pommeline/data\n",
      "Products directory: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products\n",
      "Policies directory: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies\n"
     ]
    }
   ],
   "source": [
    "# Define data directories\n",
    "data_dir = pathlib.Path().absolute().parent / \"data\"\n",
    "products_dir = data_dir / \"products\"\n",
    "policies_dir = data_dir / \"policies\"\n",
    "\n",
    "print(f\"Loading documents from: {data_dir}\")\n",
    "print(f\"Products directory: {products_dir}\")\n",
    "print(f\"Policies directory: {policies_dir}\")\n",
    "\n",
    "# Check if directories exist\n",
    "if not products_dir.exists():\n",
    "    logger.warning(f\"Products directory not found: {products_dir}\")\n",
    "if not policies_dir.exists():\n",
    "    logger.warning(f\"Policies directory not found: {policies_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12de24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,045\", \"name\": \"file_loader\", \"levelname\": \"INFO\", \"message\": \"Loaded 3 documents from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,045 - file_loader - INFO - Loaded 3 documents from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products\n",
      "2025-10-30 03:28:52,046 - ingestion_demo - INFO - Loaded 3 product documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,047\", \"name\": \"file_loader\", \"levelname\": \"INFO\", \"message\": \"Loaded 2 documents from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,047 - file_loader - INFO - Loaded 2 documents from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies\n",
      "2025-10-30 03:28:52,047 - ingestion_demo - INFO - Loaded 2 policy documents\n",
      "2025-10-30 03:28:52,047 - ingestion_demo - INFO - Total documents loaded: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "  Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n",
      "  Content length: 2364\n",
      "  Preview: # iPhone 16 Pro\n",
      "\n",
      "The iPhone 16 Pro represents Apple's latest flagship smartphone, combining cutting-edge technology with premium design and exceptional performance.\n",
      "\n",
      "## Key Features\n",
      "\n",
      "### Display and D...\n",
      "\n",
      "Document 2:\n",
      "  Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/airpods_pro_2.md\n",
      "  Content length: 3271\n",
      "  Preview: # AirPods Pro (2nd Generation)\n",
      "\n",
      "The second generation AirPods Pro represent Apple's commitment to premium wireless audio with advanced noise cancellation and spatial audio capabilities.\n",
      "\n",
      "## Key Featur...\n",
      "\n",
      "Document 3:\n",
      "  Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/macbook_air_m3.md\n",
      "  Content length: 4708\n",
      "  Preview: # MacBook Air with M3 Chip\n",
      "\n",
      "The MacBook Air with M3 chip combines exceptional performance with incredible portability, featuring a stunning Liquid Retina display and all-day battery life in a remarkab...\n"
     ]
    }
   ],
   "source": [
    "# Load documents from both directories\n",
    "all_documents = []\n",
    "\n",
    "if products_dir.exists():\n",
    "    product_docs = load_documents_from_directory(str(products_dir))\n",
    "    all_documents.extend(product_docs)\n",
    "    logger.info(f\"Loaded {len(product_docs)} product documents\")\n",
    "\n",
    "if policies_dir.exists():\n",
    "    policy_docs = load_documents_from_directory(str(policies_dir))\n",
    "    all_documents.extend(policy_docs)\n",
    "    logger.info(f\"Loaded {len(policy_docs)} policy documents\")\n",
    "\n",
    "logger.info(f\"Total documents loaded: {len(all_documents)}\")\n",
    "\n",
    "# Display document information\n",
    "for i, doc in enumerate(all_documents[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.get('source', 'Unknown')}\")\n",
    "    print(f\"  Content length: {len(doc.get('content', ''))}\")\n",
    "    print(f\"  Preview: {doc.get('content', '')[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbfbbb",
   "metadata": {},
   "source": [
    "## Document Chunking\n",
    "\n",
    "Split documents into smaller chunks for better retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfed1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,055\", \"name\": \"chunker\", \"levelname\": \"INFO\", \"message\": \"Created 11 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,055 - chunker - INFO - Created 11 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,056\", \"name\": \"chunker\", \"levelname\": \"INFO\", \"message\": \"Created 15 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/airpods_pro_2.md\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,056 - chunker - INFO - Created 15 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/airpods_pro_2.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,057\", \"name\": \"chunker\", \"levelname\": \"INFO\", \"message\": \"Created 20 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/macbook_air_m3.md\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,057 - chunker - INFO - Created 20 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/macbook_air_m3.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,058\", \"name\": \"chunker\", \"levelname\": \"INFO\", \"message\": \"Created 30 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/student_discount.md\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,058 - chunker - INFO - Created 30 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/student_discount.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,059\", \"name\": \"chunker\", \"levelname\": \"INFO\", \"message\": \"Created 30 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/return_policy.md\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,059 - chunker - INFO - Created 30 chunks from /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/return_policy.md\n",
      "2025-10-30 03:28:52,059 - ingestion_demo - INFO - Created 106 chunks from 5 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 106\n",
      "Average chunk length: 240.2 characters\n",
      "\n",
      "Chunk 1:\n",
      "  ID: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md_chunk_0\n",
      "  Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n",
      "  Length: 164 characters\n",
      "  Preview: # iPhone 16 Pro\n",
      "\n",
      "The iPhone 16 Pro represents Apple's latest flagship smartphone, combining cutting-edge technology with premium design and exceptiona...\n",
      "\n",
      "Chunk 2:\n",
      "  ID: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md_chunk_1\n",
      "  Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n",
      "  Length: 344 characters\n",
      "  Preview: ### Display and Design\n",
      "- **6.3-inch Super Retina XDR display** with ProMotion technology\n",
      "- **Titanium construction** for enhanced durability and reduc...\n",
      "\n",
      "Chunk 3:\n",
      "  ID: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md_chunk_2\n",
      "  Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n",
      "  Length: 262 characters\n",
      "  Preview: ### Performance\n",
      "- **A18 Pro chip** with 6-core CPU and 6-core GPU\n",
      "- **Hardware-accelerated ray tracing** for console-quality gaming\n",
      "- **ProMotion tech...\n"
     ]
    }
   ],
   "source": [
    "# Initialize document chunker\n",
    "chunker = SemanticChunker(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    min_chunk_size=50,\n",
    ")\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_documents:\n",
    "    chunks = chunker.chunk_text(\n",
    "        text=doc['content'],\n",
    "        source=doc['source']\n",
    "    )\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "logger.info(f\"Created {len(all_chunks)} chunks from {len(all_documents)} documents\")\n",
    "\n",
    "# Display chunk information\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"Average chunk length: {sum(len(chunk.content) for chunk in all_chunks) / len(all_chunks):.1f} characters\")\n",
    "\n",
    "# Show first few chunks\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  ID: {chunk.chunk_id}\")\n",
    "    print(f\"  Source: {chunk.source_file}\")\n",
    "    print(f\"  Length: {len(chunk.content)} characters\")\n",
    "    print(f\"  Preview: {chunk.content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2e31c",
   "metadata": {},
   "source": [
    "## Unified Index Ingestion\n",
    "\n",
    "Use the UnifiedIndexIngestion system to store both dense and sparse vectors in the same 768-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05dd05c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Index Configuration:\n",
      "  Vector Store Index: curator-pommeline\n",
      "  Settings Index: curator-pommeline-ccb01621\n",
      "  Using Index Name: curator-pommeline\n",
      "{\"asctime\": \"2025-10-30 03:28:52,075\", \"name\": \"pinecone_index_client\", \"levelname\": \"INFO\", \"message\": \"Initialized PineconeIndexClient for dense index 'curator-pommeline' (dim: 768, metric: dotproduct)\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,075 - pinecone_index_client - INFO - Initialized PineconeIndexClient for dense index 'curator-pommeline' (dim: 768, metric: dotproduct)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,080\", \"name\": \"pinecone_vector_store\", \"levelname\": \"INFO\", \"message\": \"Connected to Pinecone Index container: {'namespaces': {'curator-pommeline': {'vectorCount': 92}, 'pommeline': {'vectorCount': 0}, '': {'vectorCount': 0}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, 'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, 'curator-pommeline-12fa085f': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}}, 'dimension': 768, 'indexFullness': 0.0, 'totalVectorCount': 92}\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,080 - pinecone_vector_store - INFO - Connected to Pinecone Index container: {'namespaces': {'curator-pommeline': {'vectorCount': 92}, 'pommeline': {'vectorCount': 0}, '': {'vectorCount': 0}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, 'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, 'curator-pommeline-12fa085f': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}}, 'dimension': 768, 'indexFullness': 0.0, 'totalVectorCount': 92}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,080\", \"name\": \"bm25_vectorizer\", \"levelname\": \"INFO\", \"message\": \"Initialized BM25Vectorizer with k1=1.2, b=0.75, fixed_dim=768\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,080 - bm25_vectorizer - INFO - Initialized BM25Vectorizer with k1=1.2, b=0.75, fixed_dim=768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,081\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Initialized UnifiedIndexIngestion: index='curator-pommeline', dim=768, id='ccb01621'\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,081 - unified_index_ingestion - INFO - Initialized UnifiedIndexIngestion: index='curator-pommeline', dim=768, id='ccb01621'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized UnifiedIndexIngestion:\n",
      "  Index: curator-pommeline\n",
      "  Ingestion ID: ccb01621\n",
      "  Vector Dimension: 768\n",
      "  BM25 Vectorizer: Initialized\n"
     ]
    }
   ],
   "source": [
    "# Import the UnifiedIndexIngestion system\n",
    "from src.ingestion.unified_index_ingestion import UnifiedIndexIngestion\n",
    "\n",
    "# Use the same index name as the vector store\n",
    "actual_index_name = vector_store.index_name\n",
    "\n",
    "print(f\"Unified Index Configuration:\")\n",
    "print(f\"  Vector Store Index: {vector_store.index_name}\")\n",
    "print(f\"  Settings Index: {settings.pinecone_index_name}\")\n",
    "print(f\"  Using Index Name: {actual_index_name}\")\n",
    "\n",
    "# Initialize the unified index ingestion system\n",
    "unified_ingestion = UnifiedIndexIngestion(\n",
    "    index_name=actual_index_name,\n",
    "    ingestion_id=index_uuid,\n",
    "    vector_dimension=DIMENSION\n",
    ")\n",
    "\n",
    "print(f\"Initialized UnifiedIndexIngestion:\")\n",
    "print(f\"  Index: {unified_ingestion.index_name}\")\n",
    "print(f\"  Ingestion ID: {unified_ingestion.ingestion_id}\")\n",
    "print(f\"  Vector Dimension: {unified_ingestion.vector_dimension}\")\n",
    "print(f\"  BM25 Vectorizer: {'Initialized' if unified_ingestion.bm25_vectorizer else 'Not initialized'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e693ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,088 - ingestion_demo - INFO - Starting unified index ingestion for 106 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,089\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Starting unified index ingestion of 106 chunks into 768-dim space\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,089 - unified_index_ingestion - INFO - Starting unified index ingestion of 106 chunks into 768-dim space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,091\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Fitting BM25 vectorizer on document corpus\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,091 - unified_index_ingestion - INFO - Fitting BM25 vectorizer on document corpus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,091\", \"name\": \"bm25_vectorizer\", \"levelname\": \"INFO\", \"message\": \"Fitting BM25Vectorizer on 106 documents\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,091 - bm25_vectorizer - INFO - Fitting BM25Vectorizer on 106 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,098\", \"name\": \"bm25_vectorizer\", \"levelname\": \"INFO\", \"message\": \"BM25Vectorizer fitted with vocabulary size: 768 (fixed_dim: 768)\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,098 - bm25_vectorizer - INFO - BM25Vectorizer fitted with vocabulary size: 768 (fixed_dim: 768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:28:52,129\", \"name\": \"embedder\", \"levelname\": \"INFO\", \"message\": \"Loading embedding model: google/embeddinggemma-300m\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:28:52,129 - embedder - INFO - Loading embedding model: google/embeddinggemma-300m\n",
      "2025-10-30 03:28:52,130 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: google/embeddinggemma-300m\n",
      "2025-10-30 03:28:59,493 - sentence_transformers.SentenceTransformer - INFO - 14 prompts are loaded, with the keys: ['query', 'document', 'BitextMining', 'Clustering', 'Classification', 'InstructionRetrieval', 'MultilabelClassification', 'PairClassification', 'Reranking', 'Retrieval', 'Retrieval-query', 'Retrieval-document', 'STS', 'Summarization']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,265\", \"name\": \"embedder\", \"levelname\": \"INFO\", \"message\": \"Model loaded successfully. Embedding dimension: 768\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,265 - embedder - INFO - Model loaded successfully. Embedding dimension: 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,559\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,559 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,560\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 8461.768\", \"operation\": \"embedding_generation\", \"value\": 8461.76791191101, \"latency_ms\": \"8461.768\", \"latency_info\": \"8461.8ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,560 - metrics - INFO - Performance: embedding_generation took 8461.768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,597\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,597 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,597\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,597 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,598\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,598 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,598\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 1: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,598 - unified_index_ingestion - INFO - Processed batch 1: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,835\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,835 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,836\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 237.117\", \"operation\": \"embedding_generation\", \"value\": 237.11681365966797, \"latency_ms\": \"237.117\", \"latency_info\": \"237.1ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,836 - metrics - INFO - Performance: embedding_generation took 237.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,852\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,852 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,853\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,853 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,853\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,853 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:00,853\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 2: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:00,853 - unified_index_ingestion - INFO - Processed batch 2: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,087\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,087 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,088\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 234.182\", \"operation\": \"embedding_generation\", \"value\": 234.18188095092773, \"latency_ms\": \"234.182\", \"latency_info\": \"234.2ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,088 - metrics - INFO - Performance: embedding_generation took 234.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,105\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,105 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,106\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,106 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,106\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,106 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,107\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 3: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,107 - unified_index_ingestion - INFO - Processed batch 3: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,315\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,315 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,315\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 208.395\", \"operation\": \"embedding_generation\", \"value\": 208.39500427246094, \"latency_ms\": \"208.395\", \"latency_info\": \"208.4ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,315 - metrics - INFO - Performance: embedding_generation took 208.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,332\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,332 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,333\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,333 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,334\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,334 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,335\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 4: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,335 - unified_index_ingestion - INFO - Processed batch 4: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,707\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,707 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,708\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 371.904\", \"operation\": \"embedding_generation\", \"value\": 371.9038963317871, \"latency_ms\": \"371.904\", \"latency_info\": \"371.9ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,708 - metrics - INFO - Performance: embedding_generation took 371.904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,722\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,722 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,723\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,723 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,723\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,723 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,723\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 5: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,723 - unified_index_ingestion - INFO - Processed batch 5: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,956\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,956 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,957\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 233.387\", \"operation\": \"embedding_generation\", \"value\": 233.38699340820312, \"latency_ms\": \"233.387\", \"latency_info\": \"233.4ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,957 - metrics - INFO - Performance: embedding_generation took 233.387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,972\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,972 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,973\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,973 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,973\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,973 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:01,974\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 6: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:01,974 - unified_index_ingestion - INFO - Processed batch 6: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,146\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,146 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,146\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 171.792\", \"operation\": \"embedding_generation\", \"value\": 171.79179191589355, \"latency_ms\": \"171.792\", \"latency_info\": \"171.8ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,146 - metrics - INFO - Performance: embedding_generation took 171.792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,165\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,165 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,166\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,166 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,166\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,166 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,167\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 7: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,167 - unified_index_ingestion - INFO - Processed batch 7: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,436\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,436 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,436\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 268.572\", \"operation\": \"embedding_generation\", \"value\": 268.5718536376953, \"latency_ms\": \"268.572\", \"latency_info\": \"268.6ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,436 - metrics - INFO - Performance: embedding_generation took 268.572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,451\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,451 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,452\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,452 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,452\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,452 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,452\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 8: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,452 - unified_index_ingestion - INFO - Processed batch 8: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,693\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,693 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,694\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 240.743\", \"operation\": \"embedding_generation\", \"value\": 240.74292182922363, \"latency_ms\": \"240.743\", \"latency_info\": \"240.7ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,694 - metrics - INFO - Performance: embedding_generation took 240.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,707\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,707 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,708\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,708 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,708\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,708 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,709\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 9: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,709 - unified_index_ingestion - INFO - Processed batch 9: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,910\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 10.000\", \"operation\": \"embedding_texts_count\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,910 - metrics - INFO - Performance: embedding_texts_count took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,910\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 201.117\", \"operation\": \"embedding_generation\", \"value\": 201.11727714538574, \"latency_ms\": \"201.117\", \"latency_info\": \"201.1ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,910 - metrics - INFO - Performance: embedding_generation took 201.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,926\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 20.000\", \"operation\": \"vectors_upserted\", \"value\": 20, \"latency_ms\": \"20.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,926 - metrics - INFO - Performance: vectors_upserted took 20.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,929\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 10.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,929 - metrics - INFO - Performance: dense_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,931\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 10.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,931 - metrics - INFO - Performance: sparse_vectors_ingested took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:02,935\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 10: dense=10, sparse=10\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:02,935 - unified_index_ingestion - INFO - Processed batch 10: dense=10, sparse=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,113\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 6.000\", \"operation\": \"embedding_texts_count\", \"value\": 6, \"latency_ms\": \"6.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,113 - metrics - INFO - Performance: embedding_texts_count took 6.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,114\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 170.260\", \"operation\": \"embedding_generation\", \"value\": 170.25995254516602, \"latency_ms\": \"170.260\", \"latency_info\": \"170.3ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,114 - metrics - INFO - Performance: embedding_generation took 170.260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,127\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vectors_upserted took 12.000\", \"operation\": \"vectors_upserted\", \"value\": 12, \"latency_ms\": \"12.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,127 - metrics - INFO - Performance: vectors_upserted took 12.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,128\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: dense_vectors_ingested took 6.000\", \"operation\": \"dense_vectors_ingested\", \"value\": 6, \"latency_ms\": \"6.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,128 - metrics - INFO - Performance: dense_vectors_ingested took 6.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,128\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: sparse_vectors_ingested took 6.000\", \"operation\": \"sparse_vectors_ingested\", \"value\": 6, \"latency_ms\": \"6.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,128 - metrics - INFO - Performance: sparse_vectors_ingested took 6.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,129\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Processed batch 11: dense=6, sparse=6\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,129 - unified_index_ingestion - INFO - Processed batch 11: dense=6, sparse=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,131\", \"name\": \"bm25_vectorizer\", \"levelname\": \"INFO\", \"message\": \"BM25Vectorizer saved to data/models/bm25_ccb01621.pkl\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,131 - bm25_vectorizer - INFO - BM25Vectorizer saved to data/models/bm25_ccb01621.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,131\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"BM25 vectorizer saved for ingestion ID: ccb01621\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,131 - unified_index_ingestion - INFO - BM25 vectorizer saved for ingestion ID: ccb01621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,131\", \"name\": \"unified_index_ingestion\", \"levelname\": \"INFO\", \"message\": \"Unified index ingestion completed: {'dense_vectors': 106, 'sparse_vectors': 106, 'failed': 0}\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,131 - unified_index_ingestion - INFO - Unified index ingestion completed: {'dense_vectors': 106, 'sparse_vectors': 106, 'failed': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,132\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: unified_index_ingestion_total took 212.000\", \"operation\": \"unified_index_ingestion_total\", \"value\": 212, \"latency_ms\": \"212.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,132 - metrics - INFO - Performance: unified_index_ingestion_total took 212.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:03,132\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: unified_index_ingestion took 11043.119\", \"operation\": \"unified_index_ingestion\", \"value\": 11043.118715286255, \"latency_ms\": \"11043.119\", \"latency_info\": \"11043.1ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,132 - metrics - INFO - Performance: unified_index_ingestion took 11043.119\n",
      "2025-10-30 03:29:03,133 - ingestion_demo - INFO - Unified index ingestion completed successfully!\n",
      "2025-10-30 03:29:03,133 - ingestion_demo - INFO -   Dense vectors stored: 106\n",
      "2025-10-30 03:29:03,133 - ingestion_demo - INFO -   Sparse vectors stored: 106\n",
      "2025-10-30 03:29:03,134 - ingestion_demo - INFO -   Failed chunks: 0\n",
      "2025-10-30 03:29:03,134 - ingestion_demo - INFO -   Total vectors: 212\n",
      "2025-10-30 03:29:03,134 - ingestion_demo - INFO - Set current_ingestion_id to: ccb01621\n",
      "2025-10-30 03:29:03,134 - ingestion_demo - INFO - This enables BM25 keyword search functionality\n",
      "2025-10-30 03:29:03,135 - ingestion_demo - INFO - BM25 vectorizer successfully registered and retrievable\n"
     ]
    }
   ],
   "source": [
    "# Perform unified index ingestion with both dense and sparse vectors\n",
    "logger.info(f\"Starting unified index ingestion for {len(all_chunks)} chunks\")\n",
    "\n",
    "ingestion_result = unified_ingestion.ingest_documents(all_chunks)\n",
    "\n",
    "# Extract results from the dictionary\n",
    "dense_count = ingestion_result.get(\"dense_vectors\", 0)\n",
    "sparse_count = ingestion_result.get(\"sparse_vectors\", 0)\n",
    "failed_count = ingestion_result.get(\"failed\", 0)\n",
    "\n",
    "logger.info(f\"Unified index ingestion completed successfully!\")\n",
    "logger.info(f\"  Dense vectors stored: {dense_count}\")\n",
    "logger.info(f\"  Sparse vectors stored: {sparse_count}\")\n",
    "logger.info(f\"  Failed chunks: {failed_count}\")\n",
    "logger.info(f\"  Total vectors: {dense_count + sparse_count}\")\n",
    "\n",
    "# Set the current ingestion ID so BM25 retrieval works\n",
    "settings.current_ingestion_id = unified_ingestion.ingestion_id\n",
    "logger.info(f\"Set current_ingestion_id to: {settings.current_ingestion_id}\")\n",
    "logger.info(\"This enables BM25 keyword search functionality\")\n",
    "\n",
    "# Verify the BM25 vectorizer was registered\n",
    "from src.retrieval.bm25_vectorizer import get_bm25_vectorizer\n",
    "test_vectorizer = get_bm25_vectorizer(unified_ingestion.ingestion_id)\n",
    "if test_vectorizer:\n",
    "    logger.info(\"BM25 vectorizer successfully registered and retrievable\")\n",
    "else:\n",
    "    logger.error(\"BM25 vectorizer registration failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16edbc3",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Verify that the documents were successfully ingested by checking the index statistics and performing a test search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81e5338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Index Statistics:\n",
      "  total_documents: 0\n",
      "  embedding_dimension: 768\n",
      "  index_name: curator-pommeline\n",
      "  index_fullness: 0\n",
      "  index_type: pinecone_index_container\n",
      "  namespaces: {'curator-pommeline': {'vectorCount': 304}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, '': {'vectorCount': 0}, 'pommeline': {'vectorCount': 0}, 'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, 'curator-pommeline-12fa085f': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}}\n",
      "\n",
      "Detailed namespace analysis:\n",
      "  Namespace 'curator-pommeline': 304 vectors\n"
     ]
    }
   ],
   "source": [
    "# Get final index statistics\n",
    "final_stats = vector_store.get_stats()\n",
    "print(\"\\nFinal Index Statistics:\")\n",
    "for key, value in final_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check what's in the index directly\n",
    "print(f\"\\nDetailed namespace analysis:\")\n",
    "namespaces = final_stats.get(\"namespaces\", {})\n",
    "for ns_name, ns_data in namespaces.items():\n",
    "    vector_count = ns_data.get(\"vectorCount\", 0)\n",
    "    if vector_count > 0:\n",
    "        print(f\"  Namespace '{ns_name}': {vector_count} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44533dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Search Functionality:\n",
      "==================================================\n",
      "{\"asctime\": \"2025-10-30 03:29:03,151\", \"name\": \"embedder\", \"levelname\": \"INFO\", \"message\": \"Loading embedding model: google/embeddinggemma-300m\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:03,151 - embedder - INFO - Loading embedding model: google/embeddinggemma-300m\n",
      "2025-10-30 03:29:03,153 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: google/embeddinggemma-300m\n",
      "2025-10-30 03:29:10,425 - sentence_transformers.SentenceTransformer - INFO - 14 prompts are loaded, with the keys: ['query', 'document', 'BitextMining', 'Clustering', 'Classification', 'InstructionRetrieval', 'MultilabelClassification', 'PairClassification', 'Reranking', 'Retrieval', 'Retrieval-query', 'Retrieval-document', 'STS', 'Summarization']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,605\", \"name\": \"embedder\", \"levelname\": \"INFO\", \"message\": \"Model loaded successfully. Embedding dimension: 768\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,605 - embedder - INFO - Model loaded successfully. Embedding dimension: 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,724\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,724 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,724\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 7573.320\", \"operation\": \"embedding_generation\", \"value\": 7573.319911956787, \"latency_ms\": \"7573.320\", \"latency_info\": \"7573.3ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,724 - metrics - INFO - Performance: embedding_generation took 7573.320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,795\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,795 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,795\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: search_results_count took 3.000\", \"operation\": \"search_results_count\", \"value\": 3, \"latency_ms\": \"3.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,795 - metrics - INFO - Performance: search_results_count took 3.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,796\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vector_search took 7644.737\", \"operation\": \"vector_search\", \"value\": 7644.736766815186, \"latency_ms\": \"7644.737\", \"latency_info\": \"7644.7ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,796 - metrics - INFO - Performance: vector_search took 7644.737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'iPhone 16 Pro features'\n",
      "Results found: 3\n",
      "  1. Score: 0.6314\n",
      "     Source: data/products/iphone_16_pro.md\n",
      "     Preview: # iPhone 16 Pro\n",
      "\n",
      "The iPhone 16 Pro represents Apple's latest flagship smartphone, combining cutting-...\n",
      "  2. Score: 0.6314\n",
      "     Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n",
      "     Preview: # iPhone 16 Pro\n",
      "\n",
      "The iPhone 16 Pro represents Apple's latest flagship smartphone, combining cutting-...\n",
      "  3. Score: 0.5547\n",
      "     Source: data/products/iphone_16_pro.md\n",
      "     Preview: ## Pricing and Availability\n",
      "\n",
      "The iPhone 16 Pro is available starting at $999 for the 128GB model, wi...\n",
      "{\"asctime\": \"2025-10-30 03:29:10,833\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,833 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,834\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 37.307\", \"operation\": \"embedding_generation\", \"value\": 37.3072624206543, \"latency_ms\": \"37.307\", \"latency_info\": \"37.3ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,834 - metrics - INFO - Performance: embedding_generation took 37.307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,838\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,838 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,839\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: search_results_count took 3.000\", \"operation\": \"search_results_count\", \"value\": 3, \"latency_ms\": \"3.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,839 - metrics - INFO - Performance: search_results_count took 3.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:10,839\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vector_search took 43.162\", \"operation\": \"vector_search\", \"value\": 43.161630630493164, \"latency_ms\": \"43.162\", \"latency_info\": \"43.2ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:10,839 - metrics - INFO - Performance: vector_search took 43.162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'MacBook Air M3 performance'\n",
      "Results found: 3\n",
      "  1. Score: 0.6526\n",
      "     Source: data/products/macbook_air_m3.md\n",
      "     Preview: # MacBook Air with M3 Chip\n",
      "\n",
      "The MacBook Air with M3 chip combines exceptional performance with incre...\n",
      "  2. Score: 0.6526\n",
      "     Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/macbook_air_m3.md\n",
      "     Preview: # MacBook Air with M3 Chip\n",
      "\n",
      "The MacBook Air with M3 chip combines exceptional performance with incre...\n",
      "  3. Score: 0.5635\n",
      "     Source: data/products/macbook_air_m3.md\n",
      "     Preview: ### M3 Chip\n",
      "- **8-core CPU** with 4 performance cores and 4 efficiency cores\n",
      "- **8-core GPU** for sm...\n",
      "{\"asctime\": \"2025-10-30 03:29:11,181\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,181 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,181\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 341.395\", \"operation\": \"embedding_generation\", \"value\": 341.39513969421387, \"latency_ms\": \"341.395\", \"latency_info\": \"341.4ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,181 - metrics - INFO - Performance: embedding_generation took 341.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,188\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,188 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,189\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: search_results_count took 3.000\", \"operation\": \"search_results_count\", \"value\": 3, \"latency_ms\": \"3.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,189 - metrics - INFO - Performance: search_results_count took 3.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,189\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vector_search took 349.491\", \"operation\": \"vector_search\", \"value\": 349.4911193847656, \"latency_ms\": \"349.491\", \"latency_info\": \"349.5ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,189 - metrics - INFO - Performance: vector_search took 349.491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Student discount policy'\n",
      "Results found: 3\n",
      "  1. Score: 0.6160\n",
      "     Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/student_discount.md\n",
      "     Preview: # Student Discount Program\n",
      "\n",
      "We offer exclusive educational pricing for students, teachers, and educa...\n",
      "  2. Score: 0.5259\n",
      "     Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/student_discount.md\n",
      "     Preview: ### iPad and iPhone\n",
      "- **iPad Pro**: Up to $100 discount\n",
      "- **iPad Air**: Up to $50 discount\n",
      "- **iPad*...\n",
      "  3. Score: 0.5233\n",
      "     Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/student_discount.md\n",
      "     Preview: available throughout the year, with special promotions during back-to-school season.\n",
      "\n",
      "**Q: Can I com...\n",
      "{\"asctime\": \"2025-10-30 03:29:11,272\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,272 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,272\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 82.817\", \"operation\": \"embedding_generation\", \"value\": 82.81683921813965, \"latency_ms\": \"82.817\", \"latency_info\": \"82.8ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,272 - metrics - INFO - Performance: embedding_generation took 82.817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,278\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,278 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,279\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: search_results_count took 3.000\", \"operation\": \"search_results_count\", \"value\": 3, \"latency_ms\": \"3.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,279 - metrics - INFO - Performance: search_results_count took 3.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,279\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: vector_search took 89.825\", \"operation\": \"vector_search\", \"value\": 89.82491493225098, \"latency_ms\": \"89.825\", \"latency_info\": \"89.8ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,279 - metrics - INFO - Performance: vector_search took 89.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Return policy for electronics'\n",
      "Results found: 3\n",
      "  1. Score: 0.6778\n",
      "     Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/return_policy.md\n",
      "     Preview: ### Return Conditions\n",
      "- **Items must be in original condition** with all original packaging and acce...\n",
      "  2. Score: 0.6381\n",
      "     Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/return_policy.md\n",
      "     Preview: ### In-Store Returns\n",
      "1. **Bring the item** to any retail store location\n",
      "2. **Present original receip...\n",
      "  3. Score: 0.6176\n",
      "     Source: /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/return_policy.md\n",
      "     Preview: # Return and Refund Policy\n",
      "\n",
      "We want you to be completely satisfied with your purchase. If you're not...\n"
     ]
    }
   ],
   "source": [
    "# Test search functionality\n",
    "test_queries = [\n",
    "    \"iPhone 16 Pro features\",\n",
    "    \"MacBook Air M3 performance\",\n",
    "    \"Student discount policy\",\n",
    "    \"Return policy for electronics\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting Search Functionality:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    results = vector_store.search(query, top_k=3)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Results found: {len(results)}\")\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results):\n",
    "        print(f\"  {i+1}. Score: {score:.4f}\")\n",
    "        print(f\"     Source: {doc['source_file']}\")\n",
    "        print(f\"     Preview: {doc['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf420ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,297\", \"name\": \"cache\", \"levelname\": \"INFO\", \"message\": \"Started cache cleanup task with 300s interval\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,297 - cache - INFO - Started cache cleanup task with 300s interval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Retrieve Tool:\n",
      "========================================\n",
      "Current Configuration:\n",
      "  Current ingestion ID: 'ccb01621'\n",
      "  Expected ingestion ID: 'ccb01621'\n",
      "  Match: True\n",
      "  Index stats: {'total_documents': 0, 'embedding_dimension': 768, 'index_name': 'curator-pommeline', 'index_fullness': 0, 'index_type': 'pinecone_index_container', 'namespaces': {'': {'vectorCount': 0}, 'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, 'curator-pommeline': {'vectorCount': 304}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}, 'curator-pommeline-12fa085f': {'vectorCount': 0}, 'pommeline': {'vectorCount': 0}}}\n",
      "\n",
      "Query: 'iPhone 16 Pro features'\n",
      "------------------------------\n",
      "{\"asctime\": \"2025-10-30 03:29:11,347\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,347 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,347\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 37.222\", \"operation\": \"embedding_generation\", \"value\": 37.22190856933594, \"latency_ms\": \"37.222\", \"latency_info\": \"37.2ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,347 - metrics - INFO - Performance: embedding_generation took 37.222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,354\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,354 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,355\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: tool_retrieve took 45.0ms\", \"operation\": \"tool_retrieve\", \"value\": 45.03011703491211, \"latency_ms\": \"45.0ms\", \"latency_info\": \"45.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,355 - metrics - INFO - Performance: tool_retrieve took 45.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic search results: 3\n",
      "  Top result: 0.6314 - data/products/iphone_16_pro.md\n",
      "{\"asctime\": \"2025-10-30 03:29:11,387\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,387 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,388\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 32.301\", \"operation\": \"embedding_generation\", \"value\": 32.30118751525879, \"latency_ms\": \"32.301\", \"latency_info\": \"32.3ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,388 - metrics - INFO - Performance: embedding_generation took 32.301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,394\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,394 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,404\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,404 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,404\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: bm25_search_results took 10.000\", \"operation\": \"bm25_search_results\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,404 - metrics - INFO - Performance: bm25_search_results took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,405\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: tool_retrieve took 49.4ms\", \"operation\": \"tool_retrieve\", \"value\": 49.43680763244629, \"latency_ms\": \"49.4ms\", \"latency_info\": \"49.4ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,405 - metrics - INFO - Performance: tool_retrieve took 49.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid search results: 2\n",
      "  Components used: {'dense': True, 'bm25': True}\n",
      "\n",
      "Query: 'MacBook Air M3 performance'\n",
      "------------------------------\n",
      "{\"asctime\": \"2025-10-30 03:29:11,440\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,440 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,440\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 34.913\", \"operation\": \"embedding_generation\", \"value\": 34.912824630737305, \"latency_ms\": \"34.913\", \"latency_info\": \"34.9ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,440 - metrics - INFO - Performance: embedding_generation took 34.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,447\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,447 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,448\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: tool_retrieve took 42.4ms\", \"operation\": \"tool_retrieve\", \"value\": 42.38319396972656, \"latency_ms\": \"42.4ms\", \"latency_info\": \"42.4ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,448 - metrics - INFO - Performance: tool_retrieve took 42.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic search results: 3\n",
      "  Top result: 0.6526 - data/products/macbook_air_m3.md\n",
      "{\"asctime\": \"2025-10-30 03:29:11,482\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,482 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,483\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 34.594\", \"operation\": \"embedding_generation\", \"value\": 34.594058990478516, \"latency_ms\": \"34.594\", \"latency_info\": \"34.6ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,483 - metrics - INFO - Performance: embedding_generation took 34.594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,490\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,490 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,494\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,494 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,495\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: bm25_search_results took 10.000\", \"operation\": \"bm25_search_results\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,495 - metrics - INFO - Performance: bm25_search_results took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,495\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: tool_retrieve took 47.0ms\", \"operation\": \"tool_retrieve\", \"value\": 47.041893005371094, \"latency_ms\": \"47.0ms\", \"latency_info\": \"47.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,495 - metrics - INFO - Performance: tool_retrieve took 47.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid search results: 3\n",
      "  Components used: {'dense': True, 'bm25': True}\n"
     ]
    }
   ],
   "source": [
    "# Test the retrieve tool with detailed debugging\n",
    "from src.tools.retrieve import retrieve_documents\n",
    "from src.config import settings\n",
    "\n",
    "print(\"\\nTesting Retrieve Tool:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check current configuration\n",
    "print(f\"Current Configuration:\")\n",
    "print(f\"  Current ingestion ID: '{settings.current_ingestion_id}'\")\n",
    "print(f\"  Expected ingestion ID: '{unified_ingestion.ingestion_id}'\")\n",
    "print(f\"  Match: {settings.current_ingestion_id == unified_ingestion.ingestion_id}\")\n",
    "\n",
    "# Check index stats\n",
    "vector_store_stats = vector_store.get_stats()\n",
    "print(f\"  Index stats: {vector_store_stats}\")\n",
    "\n",
    "for query in test_queries[:2]:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Test with semantic mode first\n",
    "    response = retrieve_documents(query, top_k=3, search_mode=\"semantic\")\n",
    "    \n",
    "    print(f\"Semantic search results: {response.total_results}\")\n",
    "    \n",
    "    if response.total_results > 0:\n",
    "        print(f\"  Top result: {response.results[0].score:.4f} - {response.results[0].source_file}\")\n",
    "    else:\n",
    "        print(\"  No semantic results found\")\n",
    "    \n",
    "    # Test hybrid mode if semantic works\n",
    "    if response.total_results > 0:\n",
    "        hybrid_response = retrieve_documents(query, top_k=5, search_mode=\"hybrid\")\n",
    "        print(f\"Hybrid search results: {hybrid_response.total_results}\")\n",
    "        print(f\"  Components used: {hybrid_response.search_metadata.get('components_used', {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb52d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Search Modes Separately:\n",
      "==================================================\n",
      "\n",
      "1. Testing DENSE search only:\n",
      "{\"asctime\": \"2025-10-30 03:29:11,541\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,541 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,542\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 39.720\", \"operation\": \"embedding_generation\", \"value\": 39.72005844116211, \"latency_ms\": \"39.720\", \"latency_info\": \"39.7ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,542 - metrics - INFO - Performance: embedding_generation took 39.720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,548\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,548 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,549\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: tool_retrieve took 47.2ms\", \"operation\": \"tool_retrieve\", \"value\": 47.151803970336914, \"latency_ms\": \"47.2ms\", \"latency_info\": \"47.2ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,549 - metrics - INFO - Performance: tool_retrieve took 47.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Results: 5\n",
      "   Components used: {'dense': True, 'bm25': False}\n",
      "     1. Score: 0.6314 - data/products/iphone_16_pro.md\n",
      "     2. Score: 0.5547 - data/products/iphone_16_pro.md\n",
      "     3. Score: 0.5359 - /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/macbook_air_m3.md\n",
      "     4. Score: 0.5185 - data/products/iphone_16_pro.md\n",
      "     5. Score: 0.4913 - data/products/iphone_16_pro.md\n",
      "\n",
      "2. Testing BM25 search only:\n",
      "{\"asctime\": \"2025-10-30 03:29:11,554\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,554 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,555\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: bm25_search_results took 10.000\", \"operation\": \"bm25_search_results\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,555 - metrics - INFO - Performance: bm25_search_results took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,556\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: tool_retrieve took 5.5ms\", \"operation\": \"tool_retrieve\", \"value\": 5.536079406738281, \"latency_ms\": \"5.5ms\", \"latency_info\": \"5.5ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,556 - metrics - INFO - Performance: tool_retrieve took 5.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Results: 4\n",
      "   Components used: {'dense': False, 'bm25': True}\n",
      "     1. Score: 0.7811 - /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n",
      "     2. Score: 0.4184 - /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n",
      "     3. Score: 0.1712 - /Users/aamirsyedaltaf/Documents/curator-pommeline/data/policies/student_discount.md\n",
      "     4. Score: 0.1635 - data/products/macbook_air_m3.md\n",
      "\n",
      "3. Testing HYBRID search:\n",
      "{\"asctime\": \"2025-10-30 03:29:11,600\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_texts_count took 1.000\", \"operation\": \"embedding_texts_count\", \"value\": 1, \"latency_ms\": \"1.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,600 - metrics - INFO - Performance: embedding_texts_count took 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,601\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: embedding_generation took 44.738\", \"operation\": \"embedding_generation\", \"value\": 44.738054275512695, \"latency_ms\": \"44.738\", \"latency_info\": \"44.7ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,601 - metrics - INFO - Performance: embedding_generation took 44.738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,607\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,607 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,612\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: queries_performed took 1.0ms\", \"operation\": \"queries_performed\", \"value\": 1, \"latency_ms\": \"1.0ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,612 - metrics - INFO - Performance: queries_performed took 1.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,613\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: bm25_search_results took 10.000\", \"operation\": \"bm25_search_results\", \"value\": 10, \"latency_ms\": \"10.000\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,613 - metrics - INFO - Performance: bm25_search_results took 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-10-30 03:29:11,613\", \"name\": \"metrics\", \"levelname\": \"INFO\", \"message\": \"Performance: tool_retrieve took 57.4ms\", \"operation\": \"tool_retrieve\", \"value\": 57.39283561706543, \"latency_ms\": \"57.4ms\", \"latency_info\": \"57.4ms\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 03:29:11,613 - metrics - INFO - Performance: tool_retrieve took 57.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1. Score: 0.9836 - data/products/iphone_16_pro.md\n",
      "     2. Score: 0.9677 - /Users/aamirsyedaltaf/Documents/curator-pommeline/data/products/iphone_16_pro.md\n",
      "\n",
      "Current Configuration:\n",
      "   Current ingestion ID: 'ccb01621'\n",
      "   Expected ingestion ID: 'ccb01621'\n",
      "   Match: True\n"
     ]
    }
   ],
   "source": [
    "# Test each search mode separately for debugging\n",
    "print(\"\\nTesting Search Modes Separately:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_query = \"iPhone 16 Pro features\"\n",
    "\n",
    "# Test 1: Dense search only\n",
    "print(f\"\\n1. Testing DENSE search only:\")\n",
    "response = retrieve_documents(test_query, top_k=5, search_mode=\"semantic\")\n",
    "print(f\"   Results: {response.total_results}\")\n",
    "print(f\"   Components used: {response.search_metadata.get('components_used', {})}\")\n",
    "if response.results:\n",
    "    for i, doc in enumerate(response.results):\n",
    "        print(f\"     {i+1}. Score: {doc.score:.4f} - {doc.source_file}\")\n",
    "\n",
    "# Test 2: BM25 search only  \n",
    "print(f\"\\n2. Testing BM25 search only:\")\n",
    "response = retrieve_documents(test_query, top_k=5, search_mode=\"keyword\")\n",
    "print(f\"   Results: {response.total_results}\")\n",
    "print(f\"   Components used: {response.search_metadata.get('components_used', {})}\")\n",
    "if response.results:\n",
    "    for i, doc in enumerate(response.results):\n",
    "        print(f\"     {i+1}. Score: {doc.score:.4f} - {doc.source_file}\")\n",
    "\n",
    "# Test 3: Hybrid search\n",
    "print(f\"\\n3. Testing HYBRID search:\")\n",
    "response = retrieve_documents(test_query, top_k=5, search_mode=\"hybrid\")\n",
    "if response.results:\n",
    "    for i, doc in enumerate(response.results):\n",
    "        print(f\"     {i+1}. Score: {doc.score:.4f} - {doc.source_file}\")\n",
    "else:\n",
    "    print(\"   No results from hybrid search\")\n",
    "\n",
    "# Check ingestion ID status\n",
    "print(f\"\\nCurrent Configuration:\")\n",
    "print(f\"   Current ingestion ID: '{settings.current_ingestion_id}'\")\n",
    "print(f\"   Expected ingestion ID: '{unified_ingestion.ingestion_id}'\")\n",
    "print(f\"   Match: {settings.current_ingestion_id == unified_ingestion.ingestion_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d76f77",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Clean up the unique index created in this notebook.\n",
    "\n",
    "**WARNING**: This cell will permanently delete the unique index created in this notebook run. This ensures clean resource management and prevents leftover data in your Pinecone instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e694236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up unique index 'curator-pommeline'...\n",
      "Final statistics before cleanup:\n",
      "   Total vectors: 0\n",
      "   Namespaces: {'curator-pommeline-7b1a7bbb': {'vectorCount': 0}, 'curator-pommeline-f03bab83': {'vectorCount': 0}, '': {'vectorCount': 0}, 'curator-pommeline-a9b4d456': {'vectorCount': 0}, 'curator-pommeline': {'vectorCount': 304}, 'curator-pommeline-12fa085f': {'vectorCount': 0}, 'pommeline': {'vectorCount': 0}}\n",
      "Cleanup Summary:\n",
      "   Index: curator-pommeline\n",
      "   Total vectors to delete: 304\n",
      "   Namespace 'curator-pommeline': 304 vectors\n",
      "Attempting to clear namespace 'curator-pommeline' (304 vectors)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Namespace 'curator-pommeline' cleared successfully\n",
      "\n",
      "Cleanup Summary:\n",
      "   Successfully cleared namespaces: 1\n",
      "      - curator-pommeline\n",
      "\n",
      "Cleanup process completed\n",
      "In production with Pinecone cloud, you would use:\n",
      "   pc.delete_index('curator-pommeline') to permanently delete the index\n",
      "Unique index UUID 'ccb01621' has been processed for cleanup\n",
      "Resources have been freed from your local Pinecone instance\n",
      "Cleared current_ingestion_id from settings\n"
     ]
    }
   ],
   "source": [
    "# Clean up the unique index created in this notebook\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.append(str(pathlib.Path().absolute().parent / \"src\"))\n",
    "from src.config import settings\n",
    "\n",
    "# Get the final index statistics before cleanup\n",
    "final_stats = vector_store.get_stats()\n",
    "index_name = final_stats[\"index_name\"]\n",
    "namespaces = final_stats.get(\"namespaces\", {})\n",
    "\n",
    "print(f\"Cleaning up unique index '{index_name}'...\")\n",
    "print(f\"Final statistics before cleanup:\")\n",
    "print(f\"   Total vectors: {final_stats.get('total_documents', 'unknown')}\")\n",
    "print(f\"   Namespaces: {namespaces}\")\n",
    "\n",
    "# Calculate total vectors to delete\n",
    "total_vectors_to_delete = sum(\n",
    "    ns_data.get('vectorCount', 0) \n",
    "    for ns_data in namespaces.values()\n",
    ")\n",
    "\n",
    "print(f\"Cleanup Summary:\")\n",
    "print(f\"   Index: {index_name}\")\n",
    "print(f\"   Total vectors to delete: {total_vectors_to_delete}\")\n",
    "\n",
    "for ns_name, ns_data in namespaces.items():\n",
    "    vector_count = ns_data.get('vectorCount', 0)\n",
    "    if vector_count > 0:\n",
    "        print(f\"   Namespace '{ns_name}': {vector_count} vectors\")\n",
    "\n",
    "# For Pinecone index container, clear ALL namespaces completely\n",
    "cleared_namespaces = []\n",
    "failed_namespaces = []\n",
    "\n",
    "for ns_name, ns_data in namespaces.items():\n",
    "    vector_count = ns_data.get('vectorCount', 0)\n",
    "    if vector_count > 0:\n",
    "        print(f\"Attempting to clear namespace '{ns_name}' ({vector_count} vectors)...\")\n",
    "        \n",
    "        # Use the delete API with namespace and deleteAll flag\n",
    "        delete_request = {\n",
    "            \"namespace\": ns_name,\n",
    "            \"deleteAll\": True\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{settings.pinecone_host}/vectors/delete\",\n",
    "            json=delete_request,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            cleared_namespaces.append(ns_name)\n",
    "            result = response.json()\n",
    "            print(f\"   Namespace '{ns_name}' cleared successfully\")\n",
    "            if result:\n",
    "                print(f\"   Response: {result}\")\n",
    "        else:\n",
    "            failed_namespaces.append(ns_name)\n",
    "            print(f\"   Failed to clear namespace '{ns_name}': {response.status_code}\")\n",
    "            print(f\"   Response: {response.text}\")\n",
    "\n",
    "# Cleanup Summary\n",
    "print(f\"\\nCleanup Summary:\")\n",
    "print(f\"   Successfully cleared namespaces: {len(cleared_namespaces)}\")\n",
    "for ns in cleared_namespaces:\n",
    "    print(f\"      - {ns}\")\n",
    "\n",
    "if failed_namespaces:\n",
    "    print(f\"   Failed to clear namespaces: {len(failed_namespaces)}\")\n",
    "    for ns in failed_namespaces:\n",
    "        print(f\"      - {ns}\")\n",
    "    print(f\"   Note: Manual cleanup may be required for failed namespaces\")\n",
    "\n",
    "print(f\"\\nCleanup process completed\")\n",
    "print(f\"In production with Pinecone cloud, you would use:\")\n",
    "print(f\"   pc.delete_index('{index_name}') to permanently delete the index\")\n",
    "print(f\"Unique index UUID '{index_uuid}' has been processed for cleanup\")\n",
    "print(f\"Resources have been freed from your local Pinecone instance\")\n",
    "\n",
    "# Clear the ingestion ID to prevent conflicts\n",
    "settings.current_ingestion_id = \"\"\n",
    "print(f\"Cleared current_ingestion_id from settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curator-pommeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
